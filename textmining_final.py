# -*- coding: utf-8 -*-
"""TextMining.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SE4x7FHtfmFnvx6MaFnCHmffCe9SqcLE

**Project Description:**
We will analyze data on reported incidents of UFO sightings. Utilizing data collected by an organization dedicated to this topic, we will apply topic clustering techniques to identify commonalities among these sightings and interpret the results to provide a summary of the major themes of these reports. After clustering among the full dataset, we will then focus on comparing UFO sightings in California, Arizona, and Nevada again using clustering to investigate their similarities and differences.  

**Analysis: **
We will perform topic clustering on the text column from our dataset to identify major topics of discussion. We will then use this clustering to analyze any commonalities or anomalies based on descriptors of UFO shape, size, etc. We’ll start with a cluster analysis of the full dataset, and then narrow the focus to comparing sightings exclusively in California, Nevada, and Arizona.

**Deliverables: **
We will provide the following deliverables at the end of the project:
A dataset containing reports of UFO sightings
A set of insights derived from the dataset
A short in-class presentation of our findings, discussions of their meaning, and general “lessons learned” from our project.

# Packages and Installations:
"""

#installs any packages not available by default
#!pip install gensim
#!pip install wordcloud
# %time

#importing packages neeeded for Text Analysis
import pandas as pd
import numpy as np
import nltk
import sklearn
import gensim
import re
import string
import wordcloud
import os
import pylab as pl
import requests

# %time

##Specific Text Mining Features from SKLEARN
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import Normalizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.manifold import MDS
from sklearn.decomposition import PCA
#Other specific useful packages
from wordcloud import WordCloud
from collections import Counter
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet as wn
import matplotlib.pyplot as plt
import matplotlib as mpl
from os import path
from PIL import Image
from io import BytesIO

# %time

#Downloading features from nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
# %time

"""# User Defined Functions:"""

#Flatten Function (This will collapse a list of lists into just one list)
flatten = lambda l: [item for sublist in l for item in sublist]

#Unicoder

def Unicoder(list):
  new_list = []
  for i in list:
    new = str(i)
    new.encode('utf-8')
    new_list.append(new)
  return(new_list)

#Stringer

def Stringer(list):
  new_list = []
  for i in list:
    new = str(i)
    new_list.append(new)
  return(new_list)

#StringOnly

def StringOnly(list):
  new_list = []
  for i in list:
    if isinstance(i, str):
        new_list.append(i)
  return(new_list)

#Term Vector Function
def Term_Vectors(doc):
  punc = re.compile( '[%s]' % re.escape( string.punctuation ) )
  term_vec = [ ]

  for d in doc:
      d = str(d)
      d = d.lower()
      d = punc.sub( '', d )
      term_vec.append( nltk.word_tokenize( d ) )

  return(term_vec)

#Stop Word Function
def Stop_Word(term_vec, stop_words = nltk.corpus.stopwords.words( 'english' )):

  for i in range( 0, len( term_vec ) ):
      
      term_list = [ ]

      for term in term_vec[i]:
          if term not in stop_words:
              term_list.append( term )

      term_vec[i] = term_list

  return(term_vec)

#Porter Stem Function - Lemmatizer was better

def Porter_Stem(term_vec):
  porter = nltk.stem.porter.PorterStemmer()

  for i in range( 0, len( term_vec ) ):
    for j in range( 0, len( term_vec[ i ] ) ):
      term_vec[ i ][ j ] = porter.stem( term_vec[ i ][ j ] )

  return(term_vec)

#Lemmatizer Function
def lemmatizer(term_vec):
  for i in range( 0, len( term_vec ) ):
    for j in range( 0, len( term_vec[ i ] ) ):
      try: pos = str(wn.synsets(j)[0].pos())
      except: pos = "n"
      term_vec[i][j] = str(WordNetLemmatizer().lemmatize(term_vec[i][j],pos))
  return(term_vec)

##Basic Word Cloud Function

def show_wordcloud(data, title = None, mask = None):
    wordcloud = WordCloud(
        background_color='white',
        max_words=50,
        max_font_size=40, 
        scale=3,
        mask = mask,
        random_state=1 # chosen at random by flipping a coin; it was heads
    ).generate(str(data))

    fig = plt.figure(1, figsize=(12, 12))
    plt.axis('off')
    if title: 
        fig.suptitle(title, fontsize=20)
        fig.subplots_adjust(top=2.3)

    plt.imshow(wordcloud)
    plt.show()


def stateshape(list):
  
  shapelist = []
  
  state_shapes = {"CA": 'o',
                  "NV": 'v', 
                  "AR": '^', 
                  "NM": "<", 
                  "NC": ">"}
  
  for i in list:
    shapelist.append(state_shapes[i])

  return(shapelist)

##Color Dictionary Function Based on Cluster #s

def colordict(num):
  color_dict = {}
  if num <= 7:
    color_dict[0] = "#1b9e77"
    color_dict[1] = "#d95f02"
    color_dict[2] = "#7570b3"
    color_dict[3] = "#e7298a"
    color_dict[4] = "#66a61e"
    color_dict[5] = "#e6ab02"
    color_dict[6] = "#a6761d"  
    color_dict[7] = "#666666"  
  elif num == 8:
    color_dict[0] = "#e41a1c"
    color_dict[1] = "#377eb8"
    color_dict[2] = "#4daf4a"
    color_dict[3] = "#984ea3"
    color_dict[4] = "#ff7f00"
    color_dict[5] = "#ffff33"
    color_dict[6] = "#a65628"
    color_dict[7] = "#f781bf"
    color_dict[8] = "#999999"  
  elif num >= 9:
    color_dict[0] = "#a6cee3"
    color_dict[1] = "#1f78b4"
    color_dict[2] = "#b2df8a"
    color_dict[3] = "#33a02c"
    color_dict[4] = "#fb9a99"
    color_dict[5] = "#e31a1c"
    color_dict[6] = "#fdbf6f"
    color_dict[7] = "#ff7f00"
    color_dict[8] = "#cab2d6"
    color_dict[9] = "#6a3d9a"
    color_dict[10] = "#ffff99"
    color_dict[11] = "#b15928"
    
  return(color_dict)


"""# Initial Data Importation and Cleaning:"""

#imports ufo dataset from our data.world repo
ufoset = pd.read_csv('https://query.data.world/s/t5l7slkbhurybmuxkfgncobbaknf7i')
# %time

#subsets data by selected states, removes every column but State and Text

## ALTER FOR DIFFERENT STATES HERE ##
states = ["CA","NV","AR","NM", "NC"]
subset_ufoset = ufoset.loc[ufoset['state'].isin(states)]

encounters = subset_ufoset[['text','state']]

#Word Vectors
SelectStates_Xvect = encounters['text'].values.tolist()
SelectStates_Svect = encounters['state'].values.tolist()


shapes = stateshape(encounters['state'].values.tolist())

print("Lists created.")
# %time

# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later
ranks = []

for i in range(0,len(SelectStates_Xvect)):
    ranks.append(i)

print("Rank Index Created")
# %time

"""# Begin Text Processing with Term Vectors, Stopwords, and Stemming:"""

#Creates Term Vectors for all word vectors

SelectStates_term = Term_Vectors(SelectStates_Xvect)

print("Term Vectors  Complete.")
# %time

stopword = nltk.corpus.stopwords.words('english')
custom_words = ['summary','SUMMARY',"'","-","saw", "like", "see", "could", "looked", "seen", "foot", "would"]
stopword += custom_words

print("Stop Words Created.")
# %time

#Stop Word filter for all Vectors

SelectStates_stop = Stop_Word(SelectStates_term,stopword)

print("Stop Words filter Applied to Term Vectors.")
# %time

#Lemmatizing for All Vectors
#Results look way cleaner than porter stemming

SelectStates_lem = lemmatizer(SelectStates_stop)

print("Lemmatization Complete.")
# %time

allwords_tokens = flatten(SelectStates_stop)
allwords_stemmed = flatten(SelectStates_lem)

vocab_frame = pd.DataFrame({'words': allwords_tokens}, index = allwords_stemmed)

print("Vocab Vector Complete")
# %time

"""# tfidf Vectorization & K-Means Clustering"""

#TFIDF
SelectStates_tfidf = TfidfVectorizer(SelectStates_lem, decode_error = "replace", max_features = 200000, max_df = 0.90, min_df = 0.10)



print("Tfidf Vectors Complete.")
# %time

##Document Similarity Matrices

#Converts Items into Unicode
SelectStates_Uni = Unicoder(SelectStates_lem)

#Creates Similarity Matrix
SelectStates_matrix = SelectStates_tfidf.fit_transform(SelectStates_Uni)


print("Similarity Matrices Complete.")
# %time

#Get term names

select_terms = SelectStates_tfidf.get_feature_names()

print("Term Names Complete.")
# %time

#Pairwise Similaritiy Distances Calculation

SelectStates_dist = 1 - cosine_similarity(SelectStates_matrix)

print("Pairwise Complete Distances Calculated")
# %time

##########SLOW#################

#Cluster Selection Elbow Curve

#Nc = range(1, 20)

#kmeans = [KMeans(n_clusters=i) for i in Nc]
#kmeans

#score = [kmeans[i].fit(SelectStates_matrix).score(SelectStates_matrix) for i in range(len(kmeans))]
#score

#pl.plot(Nc,score)
#pl.xlabel('Number of Clusters')
#pl.ylabel('Score')
#pl.title('Elbow Curve')

#pl.show()

# %time

## KMeans Clustering

##ALTER FOR DIFFERENT CLUSTERING HERE##
num_clusters = 5

SelectStates_kmeans = KMeans(num_clusters,random_state =0).fit(SelectStates_matrix)

print("K-Means Clustering Complete")
# %time

#Get Cluster Labels

SelectStates_clusters = SelectStates_kmeans.labels_.tolist()

print("Cluster Labels Complete.")
# %time

##Creates UFO Dataframe

UFO_dict = { 'state': SelectStates_Svect, 'rank': ranks, 'text': SelectStates_Xvect, 'cluster': SelectStates_clusters}

UFO_frame = pd.DataFrame(UFO_dict, index = [SelectStates_clusters] , columns = ['rank', 'state', 'cluster'])

# %time

## Document/Cluster Breakdown
print(UFO_frame['cluster'].value_counts())

# %time

common_words = SelectStates_kmeans.cluster_centers_.argsort()[:,-1:-7:-1]

cluster_names = {} 
cn = 0

for num, centroid in enumerate(common_words):
    cluster_sum = str(num) + ' : ' + ', '.join(select_terms[word] for word in centroid)
    print(cluster_sum)
    cluster_names[cn] = cluster_sum
    cn += 1
    
# %time

"""# Visualizations"""

##Plotting Dictionaries


#set up colors per clusters using a dict
cluster_colors = colordict(num_clusters)

#set up custom cluster names
custom_clusters = {0 : 'one moving bright object second appeared',
                1 : 'witness provides information remain totally anonymous',
                2 : 'object moving high speed flying direction',
                3 : 'large light craft shaped aircraft sound',
                4 : 'bright light object moving around'}

#Uncomment Below to apply custom names:
#cluster_names = custom_clusters

#%time

#PCA 2 components

pca = PCA(n_components=2)

pos = pca.fit_transform(SelectStates_dist)

xs, ys = pos[:, 0], pos[:, 1]

# %time

#PCA 3 components

pca = PCA(n_components=3)

pos = pca.fit_transform(SelectStates_dist)

x_s, y_s, z_s = pos[:, 0], pos[:, 1],pos[:, 2]

# %time


#create data frame that has the result of the MDS plus the cluster numbers and titles
df = pd.DataFrame(dict(x=x_s, y=y_s, z=z_s, label=SelectStates_clusters, state=encounters['state'])) 

#Exporting Final x,y,z dataframe
filepath = "~/Documents/GitHub/Blue2_HW6_UFO_Text/"   #Update as needed
file = filepath + "ufo_plot_data.csv"

df.to_csv(file)

#group by cluster
groups = df.groupby('label')


## State/Cluster Bar Plot

StateChart = pd.crosstab(df.label, df.state).rename_axis('cluster')
print(StateChart)

## 2D PLOT

#some ipython magic to show the matplotlib plots inline
# %matplotlib inline 

# set up plot
fig, ax = plt.subplots(figsize=(30, 20)) # set size
ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling

#iterate through groups to layer the plot
#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label
for name, group in groups:
  
    mk_state = group.state.values.tolist()
    
    ax.plot(group.x, group.y, marker="o", linestyle='', ms=12, 
            label=cluster_names[name], color=cluster_colors[name], 
            mec='none')
    ax.set_aspect('auto')
    ax.tick_params(\
        axis= 'x',          # changes apply to the x-axis
        which='both',      # both major and minor ticks are affected
        bottom=False,      # ticks along the bottom edge are off
        top=False,         # ticks along the top edge are off
        labelbottom=False)
    ax.tick_params(\
        axis= 'y',         # changes apply to the y-axis
        which='both',      # both major and minor ticks are affected
        left=False,      # ticks along the bottom edge are off
        top=False,         # ticks along the top edge are off
        labelleft=False)
    
ax.legend(numpoints=1)  #show legend with only 1 point

#add label in x,y position with the label as the state title
#for i in range(len(df)):
#    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['state'], size=8)  

    
    
plt.show() #show the plot

#uncomment the below to save the plot if need be
#plt.savefig('clusters_small_noaxes.png', dpi=200)

# %time

## 3D PLOT
from mpl_toolkits.mplot3d import Axes3D

#Jupyter plot options
# %matplotlib notebook
#%matplotlib inline 

# set up plot
fig = plt.figure(figsize=(10,10))
#ax = Axes3D(fig)
ax = fig.add_subplot(111, projection='3d')

#iterate through groups to layer the plot
#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label
for name, group in groups:
    ax.plot(group.x, group.y, group.z, marker="o", linestyle='', ms=3, 
            label=cluster_names[name], color=cluster_colors[name], 
            mec='none')
    ax.set_aspect('auto')
    ax.tick_params(\
        axis= 'x',          # changes apply to the x-axis
        which='both',      # both major and minor ticks are affected
        bottom=False,      # ticks along the bottom edge are off
        top=False,         # ticks along the top edge are off
        labelbottom=False)
    ax.tick_params(\
        axis= 'y',         # changes apply to the y-axis
        which='both',      # both major and minor ticks are affected
        left=False,      # ticks along the bottom edge are off
        top=False,         # ticks along the top edge are off
        labelleft=False)
    
ax.legend(numpoints=1)  #show legend with only 1 point

#add label in x,y,z position with the label as the state
#for i in range(len(df)):
#    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['z'], df.iloc[i]['state'], size=8)  

for angle in range(0, 360):
    ax.view_init(30, angle)
    plt.draw()
    plt.pause(.001)   

# %time
    
##WORD CLOUDS
    
#Basic
show_wordcloud(SelectStates_lem)


#Masked
response = requests.get("https://raw.githubusercontent.com/dgdelisss/Blue2_HW6_UFO_Text/master/ufo_mask.png")
img = Image.open(BytesIO(response.content))
img_mask = np.array(img)

show_wordcloud(SelectStates_lem, mask=img_mask)

## PLOTS
