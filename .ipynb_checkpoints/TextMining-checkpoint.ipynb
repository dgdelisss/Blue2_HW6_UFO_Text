{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZP1jVwz1lbtZ"
   },
   "source": [
    "**Project Description:**\n",
    "We will analyze data on reported incidents of UFO sightings. Utilizing data collected by an organization dedicated to this topic, we will apply topic clustering techniques to identify commonalities among these sightings and interpret the results to provide a summary of the major themes of these reports. After clustering among the full dataset, we will then focus on comparing UFO sightings in California, Arizona, and Nevada again using clustering to investigate their similarities and differences.  \n",
    "\n",
    "**Analysis: **\n",
    "We will perform topic clustering on the text column from our dataset to identify major topics of discussion. We will then use this clustering to analyze any commonalities or anomalies based on descriptors of UFO shape, size, etc. We’ll start with a cluster analysis of the full dataset, and then narrow the focus to comparing sightings exclusively in California, Nevada, and Arizona.\n",
    "\n",
    "**Deliverables: **\n",
    "We will provide the following deliverables at the end of the project:\n",
    "A dataset containing reports of UFO sightings\n",
    "A set of insights derived from the dataset\n",
    "A short in-class presentation of our findings, discussions of their meaning, and general “lessons learned” from our project. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "taTJ6Lwgz5sr"
   },
   "source": [
    "# Packages and Installations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "_pzUFQSLuSsi",
    "outputId": "b31036ed-0eb2-4507-c4b8-2c7303de8e34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\david\\anaconda3\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\david\\anaconda3\\lib\\site-packages (from gensim) (1.14.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\david\\anaconda3\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: smart_open>=1.2.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from gensim) (1.6.0)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\david\\anaconda3\\lib\\site-packages (from smart_open>=1.2.1->gensim) (2.48.0)\n",
      "Requirement already satisfied: bz2file in c:\\users\\david\\anaconda3\\lib\\site-packages (from smart_open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied: requests in c:\\users\\david\\anaconda3\\lib\\site-packages (from smart_open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied: boto3 in c:\\users\\david\\anaconda3\\lib\\site-packages (from smart_open>=1.2.1->gensim) (1.9.13)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\david\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\david\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\david\\anaconda3\\lib\\site-packages (from requests->smart_open>=1.2.1->gensim) (2018.10.15)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.13 in c:\\users\\david\\anaconda3\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim) (1.12.14)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\david\\anaconda3\\lib\\site-packages (from boto3->smart_open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\david\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.13->boto3->smart_open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.13->boto3->smart_open>=1.2.1->gensim) (2.7.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\david\\anaconda3\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\david\\anaconda3\\lib\\site-packages (from wordcloud) (1.14.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\david\\anaconda3\\lib\\site-packages (from wordcloud) (5.1.0)\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "#installs any packages not available by default\n",
    "!pip install gensim\n",
    "!pip install wordcloud\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NQsEJ6a8jp3e",
    "outputId": "6c190c88-03b5-47ec-990f-85bd8e90f4b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#importing packages neeeded for Text Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sklearn\n",
    "import gensim\n",
    "import re\n",
    "import string\n",
    "import wordcloud\n",
    "import os\n",
    "import pylab as pl\n",
    "import requests\n",
    "import random\n",
    "\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ouc9eqz9njuJ",
    "outputId": "ccf02324-8137-4191-da8f-a5e539ea9de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "##Specific Text Mining Features from SKLEARN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.decomposition import PCA\n",
    "#Other specific useful packages\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import scipy.stats as scs\n",
    "from scipy import sparse\n",
    "from scipy.stats.distributions import chi2\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "xJ7XIEoEszpS",
    "outputId": "30b2c881-1884-44ba-cc07-c987fda40927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\David\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#Downloading features from nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7Q2-WCY0CTS"
   },
   "source": [
    "# User Defined Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXqWwedL8cMc"
   },
   "outputs": [],
   "source": [
    "#Flatten Function (This will collapse a list of lists into just one list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattens an Entire list into a large string\n",
    "def pancake(list):\n",
    "    pancake = \"\"\n",
    "    for i in list:\n",
    "        pancake = pancake + \" \" + str(i)\n",
    "    return(pancake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t1yxobFMhXrZ"
   },
   "outputs": [],
   "source": [
    "#Unicoder\n",
    "\n",
    "def Unicoder(list):\n",
    "  new_list = []\n",
    "  for i in list:\n",
    "    new = str(i)\n",
    "    new.encode('utf-8')\n",
    "    new_list.append(new)\n",
    "  return(new_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKAvXZF6R05e"
   },
   "outputs": [],
   "source": [
    "#Stringer\n",
    "\n",
    "def Stringer(list):\n",
    "  new_list = []\n",
    "  for i in list:\n",
    "    new = str(i)\n",
    "    new_list.append(new)\n",
    "  return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LZ3OwllVVEyX"
   },
   "outputs": [],
   "source": [
    "#StringOnly\n",
    "\n",
    "def StringOnly(list):\n",
    "  new_list = []\n",
    "  for i in list:\n",
    "    if isinstance(i, str):\n",
    "        new_list.append(i)\n",
    "  return(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f1RrrOrszRa3"
   },
   "outputs": [],
   "source": [
    "#Term Vector Function\n",
    "def Term_Vectors(doc):\n",
    "  punc = re.compile( '[%s]' % re.escape( string.punctuation ) )\n",
    "  term_vec = [ ]\n",
    "\n",
    "  for d in doc:\n",
    "      d = str(d)\n",
    "      d = d.lower()\n",
    "      d = punc.sub( '', d )\n",
    "      term_vec.append( nltk.word_tokenize( d ) )\n",
    "\n",
    "  return(term_vec)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBd0yw1h1BPu"
   },
   "outputs": [],
   "source": [
    "#Stop Word Function\n",
    "def Stop_Word(term_vec, stop_words = nltk.corpus.stopwords.words( 'english' )):\n",
    "\n",
    "  for i in range( 0, len( term_vec ) ):\n",
    "      \n",
    "      term_list = [ ]\n",
    "\n",
    "      for term in term_vec[i]:\n",
    "          if term not in stop_words:\n",
    "              term_list.append( term )\n",
    "\n",
    "      term_vec[i] = term_list\n",
    "\n",
    "  return(term_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMRTelCwQo02"
   },
   "outputs": [],
   "source": [
    "#Porter Stem Function - Lemmatizer was better\n",
    "\n",
    "def Porter_Stem(term_vec):\n",
    "  porter = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "  for i in range( 0, len( term_vec ) ):\n",
    "    for j in range( 0, len( term_vec[ i ] ) ):\n",
    "      term_vec[ i ][ j ] = porter.stem( term_vec[ i ][ j ] )\n",
    "\n",
    "  return(term_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BwY2bLzrABj"
   },
   "outputs": [],
   "source": [
    "#Lemmatizer Function\n",
    "def lemmatizer(term_vec):\n",
    "  for i in range( 0, len( term_vec ) ):\n",
    "    for j in range( 0, len( term_vec[ i ] ) ):\n",
    "      try: pos = str(wn.synsets(j)[0].pos())\n",
    "      except: pos = \"n\"\n",
    "      term_vec[i][j] = str(WordNetLemmatizer().lemmatize(term_vec[i][j],pos))\n",
    "  return(term_vec)\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TJkQYZDM1R6x"
   },
   "outputs": [],
   "source": [
    "##Basic Word Cloud Function\n",
    "\n",
    "def show_wordcloud(data, title = None, mask = None, max_words = 500):\n",
    "    \n",
    "    cloud = WordCloud(\n",
    "        background_color='white',\n",
    "        max_words=max_words,\n",
    "        max_font_size=50, \n",
    "        scale=3,\n",
    "        mask = mask,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    " \n",
    "    plt.imshow(cloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Color Dictionary Function Based on Cluster #s\n",
    "\n",
    "def colordict(num):\n",
    "  color_dict = {}\n",
    "  if num <= 7:\n",
    "    color_dict[0] = \"#1b9e77\"\n",
    "    color_dict[1] = \"#d95f02\"\n",
    "    color_dict[2] = \"#7570b3\"\n",
    "    color_dict[3] = \"#e7298a\"\n",
    "    color_dict[4] = \"#66a61e\"\n",
    "    color_dict[5] = \"#e6ab02\"\n",
    "    color_dict[6] = \"#a6761d\"  \n",
    "    color_dict[7] = \"#666666\"  \n",
    "  elif num == 8:\n",
    "    color_dict[0] = \"#e41a1c\"\n",
    "    color_dict[1] = \"#377eb8\"\n",
    "    color_dict[2] = \"#4daf4a\"\n",
    "    color_dict[3] = \"#984ea3\"\n",
    "    color_dict[4] = \"#ff7f00\"\n",
    "    color_dict[5] = \"#ffff33\"\n",
    "    color_dict[6] = \"#a65628\"\n",
    "    color_dict[7] = \"#f781bf\"\n",
    "    color_dict[8] = \"#999999\"  \n",
    "  elif num >= 9:\n",
    "    color_dict[0] = \"#a6cee3\"\n",
    "    color_dict[1] = \"#1f78b4\"\n",
    "    color_dict[2] = \"#b2df8a\"\n",
    "    color_dict[3] = \"#33a02c\"\n",
    "    color_dict[4] = \"#fb9a99\"\n",
    "    color_dict[5] = \"#e31a1c\"\n",
    "    color_dict[6] = \"#fdbf6f\"\n",
    "    color_dict[7] = \"#ff7f00\"\n",
    "    color_dict[8] = \"#cab2d6\"\n",
    "    color_dict[9] = \"#6a3d9a\"\n",
    "    color_dict[10] = \"#ffff99\"\n",
    "    color_dict[11] = \"#b15928\"\n",
    "    \n",
    "  return(color_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChiSquareChart(obs,exp):\n",
    "                       \n",
    "    cols = obs.columns\n",
    "    axis_name = obs.axes[1].name\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for i in cols:\n",
    "        new_row =(((obs[i]-exp[i])**2)/exp[i])\n",
    "        new_row = [\"{:0.3e}\".format(x) for x in new_row ]\n",
    "        \n",
    "        df[i] = new_row\n",
    "        \n",
    "    df = df.rename_axis(axis_name, axis=\"columns\")\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Matrix_PValue(chimatrix,dof):\n",
    "    \n",
    "    Chi_Frame.iloc[0].values\n",
    "   \n",
    "    cols = chimatrix.columns\n",
    "    axis_name = chimatrix.axes[1].name\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for i in cols:\n",
    "        p = np.array(Chi_Frame[i])\n",
    "        new_val = stats.chi2.pdf(p , dof)\n",
    "        df[i] = new_row\n",
    "        \n",
    "    df = df.rename_axis(axis_name, axis=\"columns\")\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BO3Lpnc90HJc"
   },
   "source": [
    "# Initial Data Importation and Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_kzdFuYsIIU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#imports ufo dataset from our data.world repo\n",
    "ufoset = pd.read_csv('https://query.data.world/s/t5l7slkbhurybmuxkfgncobbaknf7i')\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "pDQ6U_bymFTA",
    "outputId": "2c60eff6-b787-459a-9efc-d405506a2241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists created.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#subsets data by selected states, removes every column but State and Text\n",
    "\n",
    "## ALTER FOR DIFFERENT STATES HERE ##\n",
    "states = [\"CA\",\"NV\",\"AR\",\"NM\", \"NC\"]\n",
    "\n",
    "\n",
    "subset_ufoset = ufoset.loc[ufoset['state'].isin(states)]\n",
    "\n",
    "encounters = subset_ufoset[['text','state']]\n",
    "\n",
    "#Word Vectors\n",
    "SelectStates_Xvect = encounters['text'].values.tolist()\n",
    "SelectStates_Svect = encounters['state'].values.tolist()\n",
    "\n",
    "print(\"Lists created.\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WZx6ptC7YG7s",
    "outputId": "22a9c2a3-6ca9-4978-adf6-45f576b01a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank Index Created\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
    "ranks = []\n",
    "\n",
    "for i in range(0,len(SelectStates_Xvect)):\n",
    "    ranks.append(i)\n",
    "\n",
    "print(\"Rank Index Created\")\n",
    "%time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1UN_s-8WvEz3"
   },
   "source": [
    "# Begin Text Processing with Term Vectors, Stopwords, and Stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S71Q5F-7VVXb",
    "outputId": "f5bc4c71-b950-4c1d-84c5-0c916ea70949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Vectors  Complete.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#Creates Term Vectors for all word vectors\n",
    "\n",
    "SelectStates_term = Term_Vectors(SelectStates_Xvect)\n",
    "\n",
    "print(\"Term Vectors  Complete.\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WlKaOxc1lVSR",
    "outputId": "d5dbed4f-0854-4d97-dca4-67b0bc97ed42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words Created.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "custom_words = ['summary','SUMMARY',\"'\",\"-\",\"saw\", \"like\", \"see\", \"could\", \"looked\", \"seen\", \"foot\", \"would\",\"nuforc\"]\n",
    "stopword += custom_words\n",
    "\n",
    "print(\"Stop Words Created.\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZTQqrsgLuPp_",
    "outputId": "a4c7b235-a346-471e-dc63-052c8d6a2614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop Words filter Applied to Term Vectors.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#Stop Word filter for all Vectors\n",
    "\n",
    "SelectStates_stop = Stop_Word(SelectStates_term,stopword)\n",
    "\n",
    "print(\"Stop Words filter Applied to Term Vectors.\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "rfx5NXozq7jf",
    "outputId": "c515a578-c79e-487f-9676-d34b7e215ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization Complete.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#Lemmatizing for All Vectors\n",
    "#Results look way cleaner than porter stemming\n",
    "\n",
    "SelectStates_lem = lemmatizer(SelectStates_stop)\n",
    "\n",
    "print(\"Lemmatization Complete.\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0_jxm2gtHhYy",
    "outputId": "baeed4f7-cc18-4d1a-8c03-d57f873a821c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Vector Complete\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "allwords_tokens = flatten(SelectStates_stop)\n",
    "allwords_stemmed = flatten(SelectStates_lem)\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': allwords_tokens}, index = allwords_stemmed)\n",
    "\n",
    "print(\"Vocab Vector Complete\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0JPYET_zVJp"
   },
   "source": [
    "# tfidf Vectorization & K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N2f0e9HhzR4y",
    "outputId": "20689d50-d4df-4c41-c2cb-a94425c737ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf Vectors Complete.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#TFIDF\n",
    "SelectStates_tfidf = TfidfVectorizer(SelectStates_lem, decode_error = \"replace\", max_features = 200000, max_df = 0.90, min_df = 0.10)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Tfidf Vectors Complete.\")\n",
    "%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7YOPO660Dn7h",
    "outputId": "a4d08bf5-69a3-4e88-dd41-a7fcd2d20e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Matrices Complete.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "##Document Similarity Matrices\n",
    "\n",
    "#Converts Items into Unicode\n",
    "SelectStates_Uni = Unicoder(SelectStates_lem)\n",
    "\n",
    "#Creates Similarity Matrix\n",
    "SelectStates_matrix = SelectStates_tfidf.fit_transform(SelectStates_Uni)\n",
    "\n",
    "\n",
    "print(\"Similarity Matrices Complete.\")\n",
    "%time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "9qIkHPzHfx9i",
    "outputId": "7adf9016-ae92-4f0f-f6a2-0c0ad21c6773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Names Complete.\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#Get term names\n",
    "\n",
    "select_terms = SelectStates_tfidf.get_feature_names()\n",
    "\n",
    "print(\"Term Names Complete.\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "P4O5q-NdAWHY",
    "outputId": "623b0175-8046-4696-91e3-b9c3a4460e00",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Calculated\n"
     ]
    }
   ],
   "source": [
    "chunk_size = round(2500/len(states)) \n",
    "matrix_len = SelectStates_matrix.shape[0] # Not sparse numpy.ndarray\n",
    "\n",
    "def similarity_cosine_by_chunk(start, end):\n",
    "    if end > matrix_len:\n",
    "        end = matrix_len\n",
    "    return cosine_similarity(X=SelectStates_matrix[start:end], Y=SelectStates_matrix) # scikit-learn function\n",
    "\n",
    "\n",
    "for chunk_start in range(0, matrix_len, chunk_size):\n",
    "    cosine_similarity_chunk = similarity_cosine_by_chunk(chunk_start, chunk_start+chunk_size)\n",
    "    if chunk_start == 0: SelectStates_cosine = cosine_similarity_chunk\n",
    "    else: SelectStates_cosine = np.concatenate((SelectStates_cosine, cosine_similarity_chunk), axis=0)\n",
    "\n",
    "print(\"Cosine Similarity Calculated\")\n",
    "#%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cosine Similaritiy Matrix  -- Old\n",
    "\n",
    "#SelectStates_dist = 1 - cosine_similarity(SelectStates_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Similaritiy Distance Matrix Calculated\n"
     ]
    }
   ],
   "source": [
    "##Creates Inverse Similaritiy Distance Matrix\n",
    "\n",
    "SelectStates_dist = 1 - SelectStates_cosine\n",
    "\n",
    "print(\"Inverse Similaritiy Distance Matrix Calculated\")\n",
    "#%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FVq33TlE1weL",
    "outputId": "8ad886e7-c575-4b19-9eda-c0c133d39d8c"
   },
   "outputs": [],
   "source": [
    "## KMeans Clustering\n",
    "\n",
    "##ALTER FOR DIFFERENT CLUSTERING HERE##\n",
    "num_clusters = 4\n",
    "\n",
    "SelectStates_kmeans = KMeans(num_clusters,random_state =0).fit(SelectStates_matrix)\n",
    "\n",
    "print(\"K-Means Clustering Complete\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NHu_eIGT6KJo",
    "outputId": "215225f5-becf-4c2f-8860-b66d5d3d1009"
   },
   "outputs": [],
   "source": [
    "#Get Cluster Labels\n",
    "\n",
    "SelectStates_clusters = SelectStates_kmeans.labels_.tolist()\n",
    "\n",
    "print(\"Cluster Labels Complete.\")\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFH00YtTXsr_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##Creates UFO Dataframe\n",
    "\n",
    "UFO_dict = { 'state': SelectStates_Svect, 'rank': ranks, 'text': SelectStates_Xvect, 'cluster': SelectStates_clusters}\n",
    "\n",
    "UFO_frame = pd.DataFrame(UFO_dict, index = [SelectStates_clusters] , columns = ['rank', 'state', 'cluster'])\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "1t-4A8c4ZCcv",
    "outputId": "5d485192-35de-4664-9831-ce941718b331"
   },
   "outputs": [],
   "source": [
    "## Document/Cluster Breakdown\n",
    "print(UFO_frame['cluster'].value_counts())\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "sKly3aJI8xvK",
    "outputId": "7cd81476-0b7c-4d9c-afd5-5d8399030f61"
   },
   "outputs": [],
   "source": [
    "##Most Common 15 words per cluster \n",
    "\n",
    "common_words = SelectStates_kmeans.cluster_centers_.argsort()[:,-1:-16:-1]\n",
    "\n",
    "for num, centroid in enumerate(common_words):\n",
    "    cluster_sum = str(num) + ' : ' + ', '.join(select_terms[word] for word in centroid)\n",
    "    print(cluster_sum)\n",
    "    #cluster_names[cn] = cluster_sum\n",
    "    #cn += 1\n",
    "    \n",
    "%time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VbS44WMbjvWb"
   },
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RKseR3eLrpGz"
   },
   "outputs": [],
   "source": [
    "##Plotting Dictionaries\n",
    "\n",
    "\n",
    "#set up colors per clusters using a dict\n",
    "cluster_colors = colordict(num_clusters)\n",
    "\n",
    "#set up cluster names (5 most common)\n",
    "common_words = SelectStates_kmeans.cluster_centers_.argsort()[:,-1:-6:-1]\n",
    "\n",
    "cluster_names = {} \n",
    "cn = 0\n",
    "\n",
    "for num, centroid in enumerate(common_words):\n",
    "    cluster_sum = str(num) + ' : ' + ', '.join(select_terms[word] for word in centroid)\n",
    "    print(cluster_sum)\n",
    "    cluster_names[cn] = cluster_sum\n",
    "    cn += 1\n",
    "\n",
    "\n",
    "custom_clusters = {0 : 'one moving bright object second appeared',\n",
    "                1 : 'witness provides information remain totally anonymous',\n",
    "                2 : 'object moving high speed flying direction',\n",
    "                3 : 'large light craft shaped aircraft sound',\n",
    "                4 : 'bright light object moving around'}\n",
    "\n",
    "\n",
    "#Uncomment Below to apply custom names:\n",
    "\n",
    "#cluster_names = custom_clusters\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AREZqW_4xmjH"
   },
   "outputs": [],
   "source": [
    "#PCA 2 components\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pos = pca.fit_transform(SelectStates_dist)\n",
    "\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbYj-YeJSDqF"
   },
   "outputs": [],
   "source": [
    "#PCA 3 components\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "pos = pca.fit_transform(SelectStates_dist)\n",
    "\n",
    "x_s, y_s, z_s = pos[:, 0], pos[:, 1],pos[:, 2]\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=x_s, y=y_s, z=z_s, label=SelectStates_clusters, state=encounters['state'])) \n",
    "\n",
    "#Exporting Final x,y,z dataframe\n",
    "filepath = \"~/Documents/GitHub/Blue2_HW6_UFO_Text/\"   #Update as needed\n",
    "file = filepath + \"ufo_plot_data.csv\"\n",
    "\n",
    "df.to_csv(file)\n",
    "\n",
    "#group by cluster\n",
    "groups = df.groupby('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## State/Cluster Analysis Plot\n",
    "from scipy.stats.distributions import chi2\n",
    "\n",
    "StateChart = pd.crosstab(df.state,df.label).rename_axis('state').rename_axis(\"clusters\", axis=\"columns\")\n",
    "print(\"OBSERVED VALUES\")\n",
    "print(StateChart)\n",
    "print(\"\")\n",
    "\n",
    "chi2, p, dof, ex = scs.chi2_contingency(StateChart)\n",
    "\n",
    "print(\"\")\n",
    "print(\"EXPECTED VALUES\")\n",
    "Exp_Frame=pd.DataFrame(np.round(ex,1),index =StateChart.index[0:]).rename_axis(\"clusters\", axis=\"columns\")\n",
    "print(Exp_Frame)\n",
    "print(\"\")\n",
    "print(\"ChiSqr:\",round(chi2,3),\" P value:\", format(p, '.3e'))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1118
    },
    "colab_type": "code",
    "id": "bCAcDG3kru6h",
    "outputId": "5761d0e0-2f03-4b57-e266-14c8134531e1"
   },
   "outputs": [],
   "source": [
    "## 2D PLOT\n",
    "\n",
    "#some ipython magic to show the matplotlib plots inline\n",
    "%matplotlib inline \n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(30, 20)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "  \n",
    "    mk_state = group.state.values.tolist()\n",
    "    \n",
    "    ax.plot(group.x, group.y, marker=\"o\", linestyle='', ms=5, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the state title\n",
    "#for i in range(len(df)):\n",
    "#    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['state'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)\n",
    "\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1662
    },
    "colab_type": "code",
    "id": "wDVNiI6cVygB",
    "outputId": "135e28f8-782d-4d3d-ff11-5bc303bdfa32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 3D PLOT\n",
    "\n",
    "#Jupyter plot options\n",
    "%matplotlib notebook\n",
    "#%matplotlib inline \n",
    "\n",
    "# set up plot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "#ax = Axes3D(fig)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, group.z, marker=\"o\", linestyle='', ms=3, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelbottom=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    ax.tick_params(\\\n",
    "        axis= 'z',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left=False,      # ticks along the bottom edge are off\n",
    "        top=False,         # ticks along the top edge are off\n",
    "        labelleft=False)\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y,z position with the label as the state\n",
    "#for i in range(len(df)):\n",
    "#    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['z'], df.iloc[i]['state'], size=8)  \n",
    "\n",
    "for angle in range(0, 360):\n",
    "    ax.view_init(30, angle)\n",
    "    plt.draw()\n",
    "    plt.pause(.001)\n",
    "    \n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##WORD CLOUDS\n",
    "    \n",
    "flat_vect = flatten(SelectStates_lem)\n",
    "string_vect = StringOnly(flat_vect)\n",
    "\n",
    "#long_string = pancake(string_vect)\n",
    "\n",
    "long_string = \" \".join(str(x) for x in string_vect)\n",
    "\n",
    "#Basic\n",
    "show_wordcloud(long_string)\n",
    "\n",
    "#Masked\n",
    "response = requests.get(\"https://raw.githubusercontent.com/dgdelisss/UFO_Sightings_TextMining/master/ufo_mask.png\")\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img_mask = np.array(img)\n",
    "\n",
    "show_wordcloud(long_string, mask=img_mask,max_words=200)\n",
    "\n",
    "%time"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TextMining.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
